{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install  transformers --quiet\n!pip install rouge_score -q\n!pip install deep-phonemizer -q\n!pip install opendatasets -q\n!pip install  datasets -q\n!pip uninstall -y transformers accelerate -q\n!pip install transformers accelerate -q\n!pip install evaluate -q\n!pip install pycocoevalcap -q\n!pip install huggingface_hub -q\n!pip install -U nltk \n\nclear_output()","metadata":{"id":"0GnEadFqnRB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport requests\n\nimport os\nimport evaluate\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import io, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom datasets import load_dataset\n\nfrom transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\nfrom transformers import VisionEncoderDecoderModel , ViTImageProcessor, ViTFeatureExtractor\nfrom transformers import GPT2Config , default_data_collator\nfrom transformers import GPT2TokenizerFast\nfrom transformers import pipeline,VisionEncoderDecoderConfig\n\nimport evaluate\n\nif torch.cuda.is_available():    \n\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"-VP4yTnEnf6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"id":"mWYL48qbn2jg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip   /usr/share/nltk_data/corpora/wordnet.zip -d   /usr/share/nltk_data/corpora/","metadata":{"id":"WaLVkUcRn5Tp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intializing Encoder-Decoder Model","metadata":{}},{"cell_type":"code","source":"encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\ndecoder_model = \"gpt2\"\n\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_model, decoder_model\n).to(device)","metadata":{"id":"Hn9bqHtpn5Z8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\nimage_processor = ViTImageProcessor.from_pretrained(encoder_model)","metadata":{"id":"lh5eITmRoIr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id","metadata":{"id":"qJKY4pkvoLvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmax_length = 32 \n\ntrain_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"train[:{50}%]\")\nvalid_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"validation[:{50}%]\")\ntest_ds = load_dataset(\"HuggingFaceM4/COCO\", split=f\"test[:{50}%]\")\n","metadata":{"id":"e7yABS6WoT3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_ds = train_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\nvalid_ds = valid_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)\ntest_ds = test_ds.filter(lambda item: np.array(item[\"image\"]).ndim in [3, 4], num_proc=2)","metadata":{"id":"mj3vP7hVoOz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(items):\n  pixel_values = image_processor(items[\"image\"], return_tensors=\"pt\").pixel_values.to(device)\n  targets = tokenizer([ sentence[\"raw\"] for sentence in items[\"sentences\"] ], \n                      max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(device)\n  return {'pixel_values': pixel_values, 'labels': targets[\"input_ids\"]}\n\ntrain_dataset = train_ds.with_transform(preprocess)\nvalid_dataset = valid_ds.with_transform(preprocess)\ntest_dataset  = test_ds.with_transform(preprocess)","metadata":{"id":"jIYpEnsWoe1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.stack([x['labels'] for x in batch])\n    }","metadata":{"id":"RihSn7OOof1o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\nmeteor =  evaluate.load(\"meteor\")\n\ndef compute_metrics(eval_pred):\n    preds = eval_pred.label_ids\n    labels = eval_pred.predictions\n\n    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    #rouge score\n    rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}#{k: round(v , 4) for k, v in rouge_result.items()}# \n\n    # meteor\n    meteor_result = meteor.compute(predictions=pred_str, references=labels_str)\n\n    # bleu scores\n    bleu1_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =1)\n    bleu2_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =2)\n    bleu3_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =3)\n    bleu4_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =4)\n    \n    '''#  cider\n    # convert lists to dictionaries\n    ref_captions = {image_id: [caption] for image_id, caption in enumerate(labels_str)}\n    pred_captions = {image_id: [caption] for image_id, caption in enumerate(pred_str)}\n\n    cider_scorer = Cider()\n    cider_score, cider_scores = cider_scorer.compute_score(ref_captions, pred_captions)\n    #spice\n    spice_scorer = Spice()\n    spice_score, spice_scores = spice_scorer.compute_score(ref_captions, pred_captions)'''\n    return {\n      **rouge_result, \n      \"meteor\": round(meteor_result['meteor']*100, 2) , #round(meteor_result['meteor'], 4),\n      \"bleu1\": round(bleu1_result[\"bleu\"] * 100, 4), #round(bleu1_result[\"bleu\"] , 4), \n      \"bleu2\": round(bleu2_result[\"bleu\"] * 100, 4), # round(bleu2_result[\"bleu\"], 4), \n      \"bleu3\": round(bleu3_result[\"bleu\"] * 100, 4), #round(bleu3_result[\"bleu\"] , 4), \n      \"bleu4\": round(bleu4_result[\"bleu\"]* 100 , 4),#round(bleu4_result[\"bleu\"] , 4)\n      #\"cider\": cider_score,\n      #\"spice\": spice_scores\n      }","metadata":{"id":"NqwH1hxWokPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 2 \nbatch_size = 16 ","metadata":{"id":"RU6r2STaoo41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" '''I used this to send the model to my hugging face account you can comment this \n to save the model locally \n yu need a token to send the model to hugging face'''\n    \nfrom huggingface_hub import notebook_login\nnotebook_login()","metadata":{"id":"RwVyGPgQpNKv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" '''if the last is not commented it will send\n    the check points and final model to my hugginface \n    account but you need the token'''\n    \n''' for eval_steps, logging_steps and save_steps\n If  more epochs are used I recommend  increasing their valuse because \n the trainer will save a copy of the model at each checkpoint and it doesnot \n remove the previous ones this will fillup the disk space quickly if the # of steps is\n small \n'''\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,           \n    optim = \"adamw_torch\",\n    learning_rate = 1.5e-6,\n    num_train_epochs=num_epochs,          \n    evaluation_strategy=\"steps\",         \n    eval_steps=2000,                        \n    logging_steps=2000,                     \n    save_steps=2000,                        \n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size,     \n    output_dir=\"AsmaMassad/swin-gpt2-image-captioning-coco\", # if the line below is commented change this to the local directory\n    push_to_hub=True # comment this if you want to save the model locally \n)","metadata":{"id":"sOz2mw6Ko9lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,                     \n    tokenizer=image_processor,      \n    args=training_args,              \n    compute_metrics=compute_metrics, \n    train_dataset=train_dataset,     \n    eval_dataset=valid_dataset,      \n    data_collator=collate_fn,        \n)","metadata":{"id":"2wHAsbSzpQv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef get_eval_loader(eval_dataset=None):\n  return DataLoader(valid_dataset, collate_fn=collate_fn, batch_size=batch_size)\n\ndef get_test_loader(eval_dataset=None):\n  return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n\ntrainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\ntrainer.get_eval_dataloader = get_eval_loader\ntrainer.get_test_dataloader = get_test_loader","metadata":{"id":"AJxuCn1lpVSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"8cMKwxPQpV7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{},"execution_count":null,"outputs":[]}]}