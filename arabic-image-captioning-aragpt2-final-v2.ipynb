{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''","metadata":{"execution":{"iopub.status.busy":"2023-06-05T07:37:00.246477Z","iopub.status.idle":"2023-06-05T07:37:00.247482Z","shell.execute_reply.started":"2023-06-05T07:37:00.247239Z","shell.execute_reply":"2023-06-05T07:37:00.247261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install  transformers --quiet\n!pip install rouge_score -q\n!pip install deep-phonemizer -q\n!pip install opendatasets -q\n!pip install  datasets -q\n!pip uninstall -y transformers accelerate -q\n!pip install transformers accelerate -q\n!pip install evaluate -q\n!pip install pycocoevalcap -q\n!pip install huggingface_hub -q\n!pip install -U nltk \nclear_output()","metadata":{"id":"XoCAC7-2ay-8","execution":{"iopub.status.busy":"2023-06-06T09:22:16.149791Z","iopub.execute_input":"2023-06-06T09:22:16.150244Z","iopub.status.idle":"2023-06-06T09:24:43.392081Z","shell.execute_reply.started":"2023-06-06T09:22:16.150215Z","shell.execute_reply":"2023-06-06T09:24:43.390868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:25:21.346912Z","iopub.execute_input":"2023-06-06T09:25:21.347348Z","iopub.status.idle":"2023-06-06T09:25:21.507622Z","shell.execute_reply.started":"2023-06-06T09:25:21.347314Z","shell.execute_reply":"2023-06-06T09:25:21.506622Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!unzip   /usr/share/nltk_data/corpora/wordnet.zip -d   /usr/share/nltk_data/corpora/\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:25:21.508952Z","iopub.execute_input":"2023-06-06T09:25:21.509352Z","iopub.status.idle":"2023-06-06T09:25:22.746154Z","shell.execute_reply.started":"2023-06-06T09:25:21.509320Z","shell.execute_reply":"2023-06-06T09:25:22.745113Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\n\nimport os\n\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\n#from tqdm import tqdm\n#from tqdm.auto import tqdm\n#import multiprocessing as mp\nimport nltk\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import io, transforms\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\nfrom transformers import VisionEncoderDecoderModel , ViTImageProcessor, ViTFeatureExtractor\nfrom transformers import GPT2Config , default_data_collator\nfrom transformers import GPT2TokenizerFast\nfrom transformers import pipeline,VisionEncoderDecoderConfig\n\nimport evaluate\n\nif torch.cuda.is_available():    \n\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"id":"3f9e1113","outputId":"664cfb8f-74d7-40c0-cf8d-459bde8de3bd","execution":{"iopub.status.busy":"2023-06-06T09:26:11.252498Z","iopub.execute_input":"2023-06-06T09:26:11.253267Z","iopub.status.idle":"2023-06-06T09:26:28.075985Z","shell.execute_reply.started":"2023-06-06T09:26:11.253233Z","shell.execute_reply":"2023-06-06T09:26:28.074925Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice '''","metadata":{"id":"18_4QTj0mYcs","execution":{"iopub.status.busy":"2023-06-05T21:56:30.066316Z","iopub.execute_input":"2023-06-05T21:56:30.067365Z","iopub.status.idle":"2023-06-05T21:56:30.078786Z","shell.execute_reply.started":"2023-06-05T21:56:30.067317Z","shell.execute_reply":"2023-06-05T21:56:30.074710Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\nfrom pycocoevalcap.cider.cider import Cider\\nfrom pycocoevalcap.spice.spice import Spice '"},"metadata":{}}]},{"cell_type":"code","source":"#Loading data in colab\n'''import opendatasets as od\nod.download(\n    \"https://www.kaggle.com/datasets/adityajn105/flickr8k\")'''","metadata":{"id":"EF9ni4S5dyZK","outputId":"6830b38d-1087-4eef-e390-8b893ce43d3c","execution":{"iopub.status.busy":"2023-06-05T21:56:30.080351Z","iopub.execute_input":"2023-06-05T21:56:30.081333Z","iopub.status.idle":"2023-06-05T21:56:30.093077Z","shell.execute_reply.started":"2023-06-05T21:56:30.081295Z","shell.execute_reply":"2023-06-05T21:56:30.091926Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'import opendatasets as od\\nod.download(\\n    \"https://www.kaggle.com/datasets/adityajn105/flickr8k\")'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Initializing the Encoder-Decoder Model","metadata":{"id":"6c6b51fe"}},{"cell_type":"code","source":"encoder_model = \"microsoft/swin-base-patch4-window7-224-in22k\"\ndecoder_model = \"aubmindlab/aragpt2-base\"","metadata":{"id":"84f23221","execution":{"iopub.status.busy":"2023-06-06T09:26:28.078064Z","iopub.execute_input":"2023-06-06T09:26:28.078414Z","iopub.status.idle":"2023-06-06T09:26:28.086709Z","shell.execute_reply.started":"2023-06-06T09:26:28.078379Z","shell.execute_reply":"2023-06-06T09:26:28.085624Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"id":"-odv-carZsYc","execution":{"iopub.status.busy":"2023-06-06T09:26:28.087992Z","iopub.execute_input":"2023-06-06T09:26:28.088785Z","iopub.status.idle":"2023-06-06T09:26:28.105891Z","shell.execute_reply.started":"2023-06-06T09:26:28.088743Z","shell.execute_reply":"2023-06-06T09:26:28.104867Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_model, decoder_model).to(device)","metadata":{"id":"4vzTakXXf6Bn","outputId":"106c7b68-7afd-413e-d822-939928f99c82","execution":{"iopub.status.busy":"2023-06-06T09:26:28.110938Z","iopub.execute_input":"2023-06-06T09:26:28.111241Z","iopub.status.idle":"2023-06-06T09:26:45.464413Z","shell.execute_reply.started":"2023-06-06T09:26:28.111214Z","shell.execute_reply":"2023-06-06T09:26:45.463466Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63751a245cba4b3cbc05508ed4848d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/437M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d928cad35ce9490ea65079bbf61fac28"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']\n- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c66d67cf5a44f3b44bca924edc303c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ccc8ccfd0334fcd8aff6b82760848db"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at aubmindlab/aragpt2-base and are newly initialized: ['h.0.crossattention.bias', 'h.4.crossattention.c_proj.bias', 'h.3.crossattention.masked_bias', 'h.2.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.2.crossattention.bias', 'h.1.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.bias', 'h.7.crossattention.c_attn.weight', 'h.9.crossattention.masked_bias', 'h.3.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.masked_bias', 'h.1.crossattention.bias', 'h.4.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.7.crossattention.masked_bias', 'h.5.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.5.crossattention.masked_bias', 'h.0.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.10.crossattention.bias', 'h.1.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.masked_bias', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.6.crossattention.bias', 'h.8.crossattention.c_proj.weight', 'h.7.crossattention.bias', 'h.3.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.8.ln_cross_attn.weight', 'h.2.crossattention.masked_bias', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.10.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.4.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.bias', 'h.4.crossattention.bias', 'h.6.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.1.crossattention.masked_bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.5.ln_cross_attn.weight', 'h.5.crossattention.bias', 'h.8.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.5.crossattention.q_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"#model.config","metadata":{"id":"PFUSqthKR_14","execution":{"iopub.status.busy":"2023-06-05T21:56:54.430998Z","iopub.execute_input":"2023-06-05T21:56:54.432016Z","iopub.status.idle":"2023-06-05T21:56:54.436581Z","shell.execute_reply.started":"2023-06-05T21:56:54.431980Z","shell.execute_reply":"2023-06-05T21:56:54.435350Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the Tokenizing and Feature Extractor","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2TokenizerFast.from_pretrained(decoder_model)\nimage_processor =  ViTFeatureExtractor.from_pretrained(encoder_model)","metadata":{"id":"neXCCtgTaxk3","outputId":"091214d6-6bc0-45a5-93e3-220c34e1c08d","execution":{"iopub.status.busy":"2023-06-06T09:26:45.466058Z","iopub.execute_input":"2023-06-06T09:26:45.466434Z","iopub.status.idle":"2023-06-06T09:26:46.692057Z","shell.execute_reply.started":"2023-06-06T09:26:45.466400Z","shell.execute_reply":"2023-06-06T09:26:46.691045Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a06e3d9a0044b1ea022ec77d214d7ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67c3bb52a0345548d666e5e77a7661d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e7b5b6ef69842bfab79bcd926567545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93d2bcfe478445faac384555d9dc5ff1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Modifying some of the config params\ntokenizer.pad_token = tokenizer.eos_token \nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 5\nmodel.config.num_return_sequences = 2\nmodel.config.vocab_size = model.config.decoder.vocab_size\n","metadata":{"id":"BrIJKzCztXX4","execution":{"iopub.status.busy":"2023-06-06T09:26:46.694396Z","iopub.execute_input":"2023-06-06T09:26:46.695521Z","iopub.status.idle":"2023-06-06T09:26:46.702973Z","shell.execute_reply.started":"2023-06-06T09:26:46.695482Z","shell.execute_reply":"2023-06-06T09:26:46.701627Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Dataset","metadata":{"id":"mbQv9o6HaKfx"}},{"cell_type":"code","source":"#transformimg the images and converting them to tensors\ntrain_transforms = transforms.Compose(\n    [\n        transforms.Resize((224,224)), \n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=0, \n            std=1\n        )\n   ]\n)\n","metadata":{"id":"8V4oS8jgbG7z","execution":{"iopub.status.busy":"2023-06-06T09:28:24.664445Z","iopub.execute_input":"2023-06-06T09:28:24.665519Z","iopub.status.idle":"2023-06-06T09:28:24.670783Z","shell.execute_reply.started":"2023-06-06T09:28:24.665481Z","shell.execute_reply":"2023-06-06T09:28:24.669303Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def load_document(filename):\n    file=open(filename,'r',encoding='utf-8')\n    text=file.read()\n    file.close()\n    return text","metadata":{"id":"OBxfI5JpiZtU","execution":{"iopub.status.busy":"2023-06-06T09:27:23.414282Z","iopub.execute_input":"2023-06-06T09:27:23.414685Z","iopub.status.idle":"2023-06-06T09:27:23.419826Z","shell.execute_reply.started":"2023-06-06T09:27:23.414652Z","shell.execute_reply":"2023-06-06T09:27:23.418831Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Create dataframe that shows all captions:\n#colab\n#text = load_document('Flickr8k.arabic.txt')\n#kaggle\ntext = load_document('/kaggle/input/flickr8k-arabic-captions/Flickr8k.arabic.txt')","metadata":{"id":"oRag_gJnoQ_4","execution":{"iopub.status.busy":"2023-06-06T09:27:26.402495Z","iopub.execute_input":"2023-06-06T09:27:26.402922Z","iopub.status.idle":"2023-06-06T09:27:26.433652Z","shell.execute_reply.started":"2023-06-06T09:27:26.402872Z","shell.execute_reply":"2023-06-06T09:27:26.432690Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\ndatatxt = []\nfor line in text.split('\\n'):\n    col = line.split('\\t')\n    if len(col) == 1:\n        continue\n    w = col[0].split(\"#\")\n    datatxt.append(w + [col[1].lower()])\n\ndata = pd.DataFrame(datatxt,columns=[\"image\",\"index\",\"caption\"])\ndata = data.reindex(columns =['index','image','caption'])\ndata = data[data.image != '2258277193_586949ec62.jpg']\n\nuni_filenames = np.unique(data.image.values)\n\ndata.head()","metadata":{"id":"F5RiPXRbmt6N","outputId":"951a85bf-aedb-4b96-c33c-735763b38be9","execution":{"iopub.status.busy":"2023-06-06T09:27:29.984046Z","iopub.execute_input":"2023-06-06T09:27:29.984502Z","iopub.status.idle":"2023-06-06T09:27:30.147849Z","shell.execute_reply.started":"2023-06-06T09:27:29.984461Z","shell.execute_reply":"2023-06-06T09:27:30.146733Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"  index                      image  \\\n0     0  1000268201_693b08cb0e.jpg   \n1     1  1000268201_693b08cb0e.jpg   \n2     2  1000268201_693b08cb0e.jpg   \n3     0  1001773457_577c3a7d70.jpg   \n4     1  1001773457_577c3a7d70.jpg   \n\n                                             caption  \n0                     طفلة صغيرة تتسلق إلى مسرح خشبي  \n1                  طفلة صغيرة تتسلق الدرج إلى منزلها  \n2   فتاة صغيرة في ثوب وردي تذهب إلى المقصورة الخشبية  \n3  كلب أسود وكلب ثلاثي الألوان يلعبان مع بعضهما ا...  \n4  كلب أسود وكلب أبيض ببقع بنية يحدقان في بعضهما ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>طفلة صغيرة تتسلق إلى مسرح خشبي</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>طفلة صغيرة تتسلق الدرج إلى منزلها</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>فتاة صغيرة في ثوب وردي تذهب إلى المقصورة الخشبية</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1001773457_577c3a7d70.jpg</td>\n      <td>كلب أسود وكلب ثلاثي الألوان يلعبان مع بعضهما ا...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1001773457_577c3a7d70.jpg</td>\n      <td>كلب أسود وكلب أبيض ببقع بنية يحدقان في بعضهما ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#train_df , val_df = train_test_split(data , test_size = 0.2)\n\nsplit_ratio = 0.7\nsplit_index = int(len(data) * split_ratio)\n\ntrain_df  = data[:split_index]\nval_df = data[split_index:]\n\nsplit_ratio2 = 0.5\nsplit_index2 = int(len(val_df) * split_ratio)\n\nvalidation_df = val_df[:split_index2]\ntest_df= val_df[split_index2:]","metadata":{"id":"Yr3YTOjznZYx","execution":{"iopub.status.busy":"2023-06-06T09:27:46.766806Z","iopub.execute_input":"2023-06-06T09:27:46.767183Z","iopub.status.idle":"2023-06-06T09:27:46.774461Z","shell.execute_reply.started":"2023-06-06T09:27:46.767152Z","shell.execute_reply":"2023-06-06T09:27:46.772940Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class ImgDataset(Dataset):\n    def __init__(self, df,root_dir,tokenizer,feature_extractor, transform = None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        self.tokenizer= tokenizer\n        self.feature_extractor = feature_extractor\n        self.max_length = 32\n    def __len__(self,):\n        return len(self.df)\n    def __getitem__(self,idx):\n        caption = self.df.caption.iloc[idx]\n        image = self.df.image.iloc[idx]\n        img_path = os.path.join(self.root_dir , image)\n        img = Image.open(img_path).convert(\"RGB\")\n        \n\n        if self.transform is not None:\n            img= self.transform(img)\n        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values.to(device)\n        captions = tokenizer(caption, max_length=self.max_length, \n                             padding=\"max_length\", truncation=True).input_ids\n        \n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n        return  encoding #{'pixel_values': pixel_values, 'labels':}","metadata":{"id":"VaoY3jzasTfP","execution":{"iopub.status.busy":"2023-06-06T09:27:47.244504Z","iopub.execute_input":"2023-06-06T09:27:47.245224Z","iopub.status.idle":"2023-06-06T09:27:47.254973Z","shell.execute_reply.started":"2023-06-06T09:27:47.245188Z","shell.execute_reply":"2023-06-06T09:27:47.253922Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Colab\n'''train_dataset = ImgDataset(train_df, root_dir = \"flickr8k/Images\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\nvalidation_dataset = ImgDataset(validation_df, root_dir = \"flickr8k/Images\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\ntest_dataset = ImgDataset(test_df, root_dir = \"flickr8k/Images\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\n'''\n#Kaggle\ntrain_dataset = ImgDataset(train_df, root_dir = \"/kaggle/input/flickr8kimagescaptions/flickr8k/images/\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\nvalidation_dataset = ImgDataset(validation_df, root_dir = \"/kaggle/input/flickr8kimagescaptions/flickr8k/images/\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\ntest_dataset = ImgDataset(test_df, root_dir = \"/kaggle/input/flickr8kimagescaptions/flickr8k/images/\",tokenizer=tokenizer,feature_extractor = image_processor  ,transform = train_transforms)\n","metadata":{"id":"o3K6ikZIsewr","execution":{"iopub.status.busy":"2023-06-06T09:28:42.775770Z","iopub.execute_input":"2023-06-06T09:28:42.776269Z","iopub.status.idle":"2023-06-06T09:28:42.787637Z","shell.execute_reply.started":"2023-06-06T09:28:42.776189Z","shell.execute_reply":"2023-06-06T09:28:42.786368Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for item in train_dataset:\n    print(\"train_dataset\", item[\"labels\"].shape)\n    print(\"train_dataset\", item[\"pixel_values\"].shape)\n    break\nfor item in validation_dataset:\n    print(\"validation_dataset\", item[\"labels\"].shape)\n    print(\"validation_dataset\", item[\"pixel_values\"].shape)\n    break\nfor item in test_dataset:\n    print(\"test_dataset\", item[\"labels\"].shape)\n    print(\"test_dataset\", item[\"pixel_values\"].shape)\n    break","metadata":{"id":"59cVY0BsdHvf","outputId":"e861a748-7fed-4605-dee0-9bd5b4d56bd8","execution":{"iopub.status.busy":"2023-06-06T09:28:46.306781Z","iopub.execute_input":"2023-06-06T09:28:46.307318Z","iopub.status.idle":"2023-06-06T09:28:46.394135Z","shell.execute_reply.started":"2023-06-06T09:28:46.307277Z","shell.execute_reply":"2023-06-06T09:28:46.393233Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"train_dataset torch.Size([32])\ntrain_dataset torch.Size([3, 224, 224])\nvalidation_dataset torch.Size([32])\nvalidation_dataset torch.Size([3, 224, 224])\ntest_dataset torch.Size([32])\ntest_dataset torch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training the Model","metadata":{"id":"0t1K7z8mstcu"}},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"\nrouge = evaluate.load(\"rouge\")\nbleu = evaluate.load(\"bleu\")\nmeteor =  evaluate.load(\"meteor\")\n\ndef compute_metrics(eval_pred):\n    preds = eval_pred.label_ids\n    labels = eval_pred.predictions\n\n    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    #rouge score\n    rouge_result = rouge.compute(predictions=pred_str, references=labels_str)\n    rouge_result = {k: round(v * 100, 4) for k, v in rouge_result.items()}#{k: round(v , 4) for k, v in rouge_result.items()}# \n\n    # meteor\n    meteor_result = meteor.compute(predictions=pred_str, references=labels_str)\n\n    # bleu scores\n    bleu1_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =1)\n    bleu2_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =2)\n    bleu3_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =3)\n    bleu4_result = bleu.compute(predictions=pred_str, references=labels_str, max_order =4)\n    \n    generation_length = bleu4_result[\"translation_length\"]\n    '''#  cider\n    # convert lists to dictionaries\n    ref_captions = {image_id: [caption] for image_id, caption in enumerate(labels_str)}\n    pred_captions = {image_id: [caption] for image_id, caption in enumerate(pred_str)}\n\n    cider_scorer = Cider()\n    cider_score, cider_scores = cider_scorer.compute_score(ref_captions, pred_captions)\n    #spice\n    spice_scorer = Spice()\n    spice_score, spice_scores = spice_scorer.compute_score(ref_captions, pred_captions)'''\n    return {\n      **rouge_result, \n      \"meteor\": round(meteor_result['meteor']*100, 2) , #round(meteor_result['meteor'], 4),\n      \"bleu1\": round(bleu1_result[\"bleu\"] * 100, 4), #round(bleu1_result[\"bleu\"] , 4), \n      \"bleu2\": round(bleu2_result[\"bleu\"] * 100, 4), # round(bleu2_result[\"bleu\"], 4), \n      \"bleu3\": round(bleu3_result[\"bleu\"] * 100, 4), #round(bleu3_result[\"bleu\"] , 4), \n      \"bleu4\": round(bleu4_result[\"bleu\"]* 100 , 4),#round(bleu4_result[\"bleu\"] , 4)\n      #\"cider\": cider_score,\n      #\"spice\": spice_scores\n       #\"gen_len\": bleu4_result[\"translation_length\"] / len(preds)\n      }","metadata":{"id":"JwTFZhjFc09x","outputId":"c8fa8bc0-1f08-49f0-c467-0b90f7b5ee89","execution":{"iopub.status.busy":"2023-06-06T09:59:18.198347Z","iopub.execute_input":"2023-06-06T09:59:18.198965Z","iopub.status.idle":"2023-06-06T09:59:19.439146Z","shell.execute_reply.started":"2023-06-06T09:59:18.198923Z","shell.execute_reply":"2023-06-06T09:59:19.438084Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# collate lists of samples into batches\ndef collate_fn(batch):\n    return {\n        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n        'labels': torch.stack([x['labels'] for x in batch])\n    }","metadata":{"id":"G5uNu7LgwPG4","execution":{"iopub.status.busy":"2023-06-06T09:59:28.383422Z","iopub.execute_input":"2023-06-06T09:59:28.384125Z","iopub.status.idle":"2023-06-06T09:59:28.389509Z","shell.execute_reply.started":"2023-06-06T09:59:28.384088Z","shell.execute_reply":"2023-06-06T09:59:28.388495Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nbatch_size = 16 ","metadata":{"id":"jhhKgSqfwc-f","execution":{"iopub.status.busy":"2023-06-06T09:59:28.820464Z","iopub.execute_input":"2023-06-06T09:59:28.820804Z","iopub.status.idle":"2023-06-06T09:59:28.825636Z","shell.execute_reply.started":"2023-06-06T09:59:28.820777Z","shell.execute_reply":"2023-06-06T09:59:28.824518Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,             \n    num_train_epochs=num_epochs,      \n    evaluation_strategy=\"steps\", \n    learning_rate = 1.5e-6,\n    optim = \"adamw_torch\",\n    eval_steps=2500,                     \n    logging_steps=2500,                   \n    save_steps=2500,                        \n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,  \n    output_dir=\"AsmaMassad/swin-aragpt2-image-captioning-flickr8k\",\n    load_best_model_at_end=True,\n    push_to_hub=True # push the model to huggingface hub,\n)\n","metadata":{"id":"_bzcx9HsteVU","outputId":"5f894fe9-5ce7-4c9d-9c4e-634d0cbab03e","execution":{"iopub.status.busy":"2023-06-06T09:59:29.427130Z","iopub.execute_input":"2023-06-06T09:59:29.427513Z","iopub.status.idle":"2023-06-06T09:59:29.463335Z","shell.execute_reply.started":"2023-06-06T09:59:29.427480Z","shell.execute_reply":"2023-06-06T09:59:29.462301Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-06-06T09:29:58.225114Z","iopub.execute_input":"2023-06-06T09:29:58.225463Z","iopub.status.idle":"2023-06-06T09:29:58.260292Z","shell.execute_reply.started":"2023-06-06T09:29:58.225434Z","shell.execute_reply":"2023-06-06T09:29:58.259451Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea09358bacdf43cd8a81674158f4e228"}},"metadata":{}}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,                  \n    tokenizer=image_processor,    \n    args=training_args,              \n    compute_metrics=compute_metrics, \n    train_dataset=train_dataset,     \n    eval_dataset=validation_dataset,      \n    data_collator=collate_fn,        \n)","metadata":{"id":"zuV_iD-Wt6C9","execution":{"iopub.status.busy":"2023-06-06T09:59:34.107366Z","iopub.execute_input":"2023-06-06T09:59:34.107750Z","iopub.status.idle":"2023-06-06T09:59:36.493604Z","shell.execute_reply.started":"2023-06-06T09:59:34.107718Z","shell.execute_reply":"2023-06-06T09:59:36.492646Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/kaggle/working/AsmaMassad/swin-aragpt2-image-captioning-flickr8k is already a clone of https://huggingface.co/AsmaMassad/swin-aragpt2-image-captioning-flickr8k. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef get_eval_loader(eval_dataset=None):\n    return DataLoader(validation_dataset, collate_fn=collate_fn, batch_size=batch_size)\n\ndef get_test_loader(eval_dataset=None):\n    return DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size)\n\ntrainer.get_train_dataloader = lambda: DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size)\ntrainer.get_eval_dataloader = get_eval_loader\ntrainer.get_test_dataloader = get_test_loader","metadata":{"id":"4xH1bkve0wim","execution":{"iopub.status.busy":"2023-06-06T09:59:36.495724Z","iopub.execute_input":"2023-06-06T09:59:36.496129Z","iopub.status.idle":"2023-06-06T09:59:36.502270Z","shell.execute_reply.started":"2023-06-06T09:59:36.496093Z","shell.execute_reply":"2023-06-06T09:59:36.501166Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"trainer.train( )","metadata":{"id":"uffotMCi07H1","outputId":"61da24c7-1428-487b-c05d-405cb1a03f0f","execution":{"iopub.status.busy":"2023-06-06T09:59:36.503651Z","iopub.execute_input":"2023-06-06T09:59:36.504235Z","iopub.status.idle":"2023-06-06T13:21:14.016455Z","shell.execute_reply.started":"2023-06-06T09:59:36.504201Z","shell.execute_reply":"2023-06-06T13:21:14.015526Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10620' max='10620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10620/10620 3:21:35, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Meteor</th>\n      <th>Bleu1</th>\n      <th>Bleu2</th>\n      <th>Bleu3</th>\n      <th>Bleu4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2500</td>\n      <td>1.463700</td>\n      <td>1.288443</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.960000</td>\n      <td>9.530600</td>\n      <td>3.425800</td>\n      <td>1.499400</td>\n      <td>0.584400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.320100</td>\n      <td>1.230201</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.330000</td>\n      <td>8.672300</td>\n      <td>3.194400</td>\n      <td>1.432900</td>\n      <td>0.537700</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.261700</td>\n      <td>1.213090</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.680000</td>\n      <td>9.265000</td>\n      <td>3.336300</td>\n      <td>1.409900</td>\n      <td>0.519300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.228000</td>\n      <td>1.208968</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.260000</td>\n      <td>8.178800</td>\n      <td>2.877200</td>\n      <td>1.234000</td>\n      <td>0.504900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10620, training_loss=1.3162173628582567, metrics={'train_runtime': 12097.4636, 'train_samples_per_second': 14.045, 'train_steps_per_second': 0.878, 'total_flos': 3.0837741499339407e+19, 'train_loss': 1.3162173628582567, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"# evaluate on the test_dataset\ntrainer.evaluate(test_dataset)","metadata":{"id":"julgp_iuRcmD","outputId":"db1488b3-4070-4849-a2f2-4586e2d15982","execution":{"iopub.status.busy":"2023-06-06T13:21:14.027864Z","iopub.execute_input":"2023-06-06T13:21:14.028194Z","iopub.status.idle":"2023-06-06T13:28:17.402864Z","shell.execute_reply.started":"2023-06-06T13:21:14.028165Z","shell.execute_reply":"2023-06-06T13:28:17.401964Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='319' max='319' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [319/319 06:53]\n    </div>\n    "},"metadata":{}},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.2089682817459106,\n 'eval_rouge1': 0.0,\n 'eval_rouge2': 0.0,\n 'eval_rougeL': 0.0,\n 'eval_rougeLsum': 0.0,\n 'eval_meteor': 5.26,\n 'eval_bleu1': 8.1788,\n 'eval_bleu2': 2.8772,\n 'eval_bleu3': 1.234,\n 'eval_bleu4': 0.5049,\n 'eval_runtime': 423.3508,\n 'eval_samples_per_second': 12.04,\n 'eval_steps_per_second': 0.378,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"code","source":"'''beam_outputs = model.generate(\n    input_ids, \n    max_length=50, \n    num_beams=5, \n    no_repeat_ngram_size=2, \n    num_return_sequences=5, \n    early_stopping=True\n)'''","metadata":{"id":"kPkmT0E3xSi4"},"execution_count":null,"outputs":[]}]}